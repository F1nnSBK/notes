{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple DAG structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "from airflow import DAG\n",
    "\n",
    "from airflow.decorators import task\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "default_dag_args = {\n",
    "    \"start_date\": datetime.datetime(2018, 1, 1)\n",
    "}\n",
    "\n",
    "# Begin of workflow\n",
    "with DAG(\n",
    "    \"simple_greeting\",\n",
    "    schedule_interval=datetime.timedelta(days=1),\n",
    "    default_args=default_dag_args\n",
    ") as dag:\n",
    "    \n",
    "    @task()\n",
    "    def greeting():\n",
    "        import logging\n",
    "        \n",
    "        logging.info(\"Hi\")\n",
    "        \n",
    "    \n",
    "    # Task calls function greeting()\n",
    "    hello_python = PythonOperator(task_id=\"hi\", python_callable=greeting)\n",
    "    \n",
    "    goodbye_bash = BashOperator(task_id=\"bye\", bash_command=\"echo Goodbye.\")\n",
    "    \n",
    "    # Define order of tasks with << and >>\n",
    "    hello_python >> goodbye_bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bash Operator\n",
    "\n",
    "Der BashOperator entählt folgende Pakete:  \n",
    "    - gcloud command, including the gcloud storage sub-command for working with Cloud Storage buckets.  \n",
    "    - bq command   \n",
    "    - kubectl command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from airflow.operators import bash\n",
    "\n",
    "# Create BigQuery output dataset.\n",
    "make_bq_dataset = bash.BashOperator(\n",
    "    taskt_id=\"make_bq_dataset\"\n",
    "    \n",
    "    bash_command=f\"bq ls {bq_dataset_name} || bq mk {bq_dataset_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PythonOperator\n",
    "\n",
    "Der Python Code wird bei Cloud Composer in einem container mit dem [Cloud Composer Image](https://cloud.google.com/composer/docs/concepts/versioning/composer-versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python import PythonOperator\n",
    "    \n",
    "@task()\n",
    "def greeting():\n",
    "    import logging\n",
    "    \n",
    "    logging.info(\"Hi\")\n",
    "    \n",
    "# Task calls greeting() function\n",
    "hello_python = PythonOperator(task_id=\"hi\", python_callable=greeting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Cloud Operators\n",
    "\n",
    "Es gibt sehr viele [Google Cloud Operatoren](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/operators/cloud/index.html).  \n",
    "\n",
    "Der Link führt zur Doku.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EmailOperator\n",
    "\n",
    "Was macht der wohl?  \n",
    "Benötigt weiter Konfiguration im Composer environment.  \n",
    "\n",
    "[12.000 Kostenlose Emails im Monat](https://cloud.google.com/composer/docs/configure-email)   (Anleitung zu Konfiguration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators import email\n",
    "\n",
    "min_query_date = 0\n",
    "max_query_date = 0 \n",
    "output_file=\"path/to/output/file\"\n",
    "\n",
    "# Send email confirmation \n",
    "email_summary = email.EmailOperator(\n",
    "    task_id=\"email_summary\",\n",
    "    to=\"{{var.value.email}}\",\n",
    "    subject=\"Sample BigQuery notify data ready\",\n",
    "    html_content=\"\"\"\n",
    "    Analyzed Stack Overflow posts data from {min_date} 12AM to {max_date}\n",
    "    12AM. The most popular question was '{question_title}' with\n",
    "    {view_count} views. Top 100 questions asked are now available at:\n",
    "    {export_location}.\n",
    "    \"\"\".format(\n",
    "        min_date=min_query_date,\n",
    "        max_date=max_query_date,\n",
    "        question_title=(\n",
    "            \"{{ ti.xcom_pull(task_ids='bq_read_most_popular', \"\n",
    "            \"key='return_value')[0][0] }}\"\n",
    "        ),\n",
    "        view_count=(\n",
    "            \"{{ ti.xcom_pull(task_ids='bq_read_most_popular', \"\n",
    "            \"key='return_value')[0][1] }}\"\n",
    "        ),\n",
    "        export_location=output_file,\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notification on operator failure\n",
    "\n",
    "Man kann `email_on_faile` auf `True` setzen und erhält dann eine Benachrichtigung, wenn der DAG fehlschlägt.  \n",
    "Muss in einem Composer environment ebenfalls konfiuriert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import models\n",
    "\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "yesterday = \"Gestern\"\n",
    "project_id = \"gcloud project id\"\n",
    "\n",
    "default_dag_args = {\n",
    "    \"start_date\": yesterday,\n",
    "    # Email whenever an Operator in the DAG fails.\n",
    "    \"email\":\"{{var.value.email}}\",\n",
    "    \"email_on_failure\":True,\n",
    "    \"email_on_retry\":False,\n",
    "    \"retries\":1,\n",
    "    \"retry_delay\": datetime.timedelta(minutes=5),\n",
    "    \"project_id\":project_id\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    \"composer_sample_bq_notify\",\n",
    "    schedule_interval=datetime.timedelta(weeks=4),\n",
    "    default_args=default_dag_args\n",
    ") as dag:\n",
    "    \n",
    "    @task()\n",
    "    def HelloWorld():\n",
    "        import logging\n",
    "        \n",
    "        logging.info(Hello World) # Wird schiefgehen!\n",
    "    \n",
    "    # Tasks\n",
    "    hello_world = PythonOperator(task_id=\"HelloWorld\", python_callable=\"HelloWorld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best practices\n",
    "\n",
    "+ Custom libraries  \n",
    "    Platziere keine custom libraries in das top-level des `/dags` Ordners.\n",
    "\n",
    "+ Pro Python module nur ein DAG (Auch keine SubDAGs). Davor solllen die Tasks in einem DAG gruppiert werden.\n",
    "\n",
    "+ [Teste deine DAGs](https://cloud.google.com/composer/docs/how-to/using/testing-dags)\n",
    "\n",
    "\n",
    "#### Info / Resources\n",
    "\n",
    "+ [How-to Guides](https://airflow.apache.org/docs/apache-airflow/2.9.3/howto/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group tasks inside DAGs\n",
    "\n",
    "Um tasks in einem DAG zu gruppieren, werden sogenannte relationships verwendet. \n",
    "\n",
    "\n",
    "![image.png](./DokuData/images/group_tasks_dag.png)\n",
    "\n",
    "\n",
    "Hier laufen op-1 und op-2 gleichzeitig, so wie op-3 und op-4. Das kann erreicht werden, indem man eine relationship zwischen des tasks herstellt. Abgeleitet aus der vorher beschriebenen Syntax für die Reihenfolge ergibt sich: \n",
    "\n",
    "`start >> [task_1, task_2]`  \n",
    "\n",
    "Mit den `[]` Klammern wird eine relationship / group ausgedrückt. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "DAG_NAME= \"all_tasks_in_on_dag\"\n",
    "\n",
    "args = {\"owner\":\"airflow\",\"start_date\":days_ago(1),\"schedule_interval\":\"@once\"}\n",
    "\n",
    "with DAG(dag_id=DAG_NAME, default_args=args) as dag:\n",
    "    start = EmptyOperator(task_id=\"start\")\n",
    "    \n",
    "    task_1 = BashOperator(task_id=\"op-1\", bash_command=\":\", dag=dag)\n",
    "    task_2 = BashOperator(task_id=\"op-2\", bash_command=\":\", dag=dag)\n",
    "    \n",
    "    some_other_task = BashOperator(task_id=\"some-other-task\", bash_command=\":\", dag=dag)\n",
    "    \n",
    "    task_3 = BashOperator(task_id=\"op-3\", bash_command=\":\", dag=dag)\n",
    "    task_4 = BashOperator(task_id=\"op-4\", bash_command=\":\", dag=dag)\n",
    "    \n",
    "    end = EmptyOperator(task_id=\"end\")\n",
    "    \n",
    "    start >> [task_1, task_2] >> some_other_task >> [task_3, task_4] >> end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Triggering children DAGs from a parent DAG\n",
    "\n",
    "+ `TriggerDagRunOperator`  \n",
    "\n",
    "![](./DokuData/images/trigger_child_dag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "DAG_NAME = \"trigger_child_dag\"\n",
    "\n",
    "with DAG(dag_id=DAG_NAME,\n",
    "         default_args={\"owner\":\"airflow\"},\n",
    "         start_date=days_ago(1),\n",
    "         schedule_interval=\"@once\",\n",
    ") as dag:\n",
    "    start = EmptyOperator(task_id=\"start\")\n",
    "    \n",
    "    trigger_1 = TriggerDagRunOperator(\n",
    "        task_id=\"dag_1\",\n",
    "        trigger_dag_id=\"dag-to-trigger\",\n",
    "        conf={\"message\":\"Hello World\"}\n",
    "    )\n",
    "    \n",
    "    trigger_2 = TriggerDagRunOperator(\n",
    "        task_id=\"dag_2\",\n",
    "        trigger_dag_id=\"dag-to-trigger\",\n",
    "        conf={\"message\":\"Hello World\"}\n",
    "    )\n",
    "    \n",
    "    some_other_task = EmptyOperator(task_id=\"some-other-task\")\n",
    "    \n",
    "    end = EmptyOperator(task_id=\"end\")\n",
    "    \n",
    "    start >> trigger_1 >> some_other_task >> trigger_2 >> end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "DAG_NAME = \"dag-to-start\"\n",
    "\n",
    "args = {\"owner\": \"airflow\", \"start_date\": days_ago(1), \"schedule_interval\": \"None\"}\n",
    "\n",
    "with DAG(dag_id=DAG_NAME, default_args=args) as dag:\n",
    "    dag_task = EmptyOperator(task_id=\"dag-task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Taskgroups\n",
    "\n",
    "+ Nutzt den `TaskgroupOperator``\n",
    "\n",
    "![taskgroup example](./DokuData/images/taskgroup.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.utils.task_group import TaskGroup\n",
    "\n",
    "with DAG(dag_id=\"taskgroup example\", start_date=days_ago(1)) as dag:\n",
    "    start = EmptyOperator(task_id=\"start\")\n",
    "    \n",
    "    with TaskGroup(\"taskgroup_1\", tooltip=\"task group #1\") as section_1:\n",
    "        task_1 = BashOperator(task_id=\"op-1\", bash_command=\":\")\n",
    "        task_2 = BashOperator(task_id=\"op-2\", bash_command=\":\")\n",
    "        \n",
    "    with TaskGroup(\"taskgroup_2\", tooltip=\"task group #2\") as section_2:\n",
    "        task_1 = BashOperator(task_id=\"op-3\", bash_command=\":\")\n",
    "        task_2 = BashOperator(task_id=\"op-4\", bash_command=\":\")\n",
    "        \n",
    "    some_other_task = EmptyOperator(task_id=\"some-other-task\")\n",
    "    \n",
    "    end = EmptyOperator(task_id=\"end\")\n",
    "    \n",
    "    start >> section_1 >> some_other_task >> section_2 >> end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipps & Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding tags to DAGs for filtering in the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAG(dag_id=\"example_dag_tag\", schedule=\"0 0 * * *\", tags=[\"example\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add owner links to a DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAG(dag_id=\"example_dag_tag\", schedule=\"0 0 * * *\", tags=[\"example\"], owner_links=\"sdd\":\"https://www.schwaebisch-media.de/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.notifications.basenotifier import BaseNotifier\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
